{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPAyCHcjDL4foKrbHQp2nKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alierenc/di725-transformers-and-attention-based-deep-networks/blob/main/Assignment1/2.%20Training%20nanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQsw1ay22j2c",
        "outputId": "f2da5d54-a2be-41a4-cbcc-f7ed7fbc976f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeren\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access dataset and save models\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "train_data_path = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Assignment 1/train_data.csv\"\n",
        "val_data_path = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Assignment 1/validation_data.csv\"\n",
        "test_data_path = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Assignment 1/test_data.csv\"\n",
        "\n",
        "!pip install wandb\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch import nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def load_data(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    texts = df['conversation'].tolist()\n",
        "    labels = df['customer_sentiment'].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "# Load train, validation, and test data\n",
        "train_texts, train_labels = load_data(train_data_path)\n",
        "val_texts, val_labels = load_data(val_data_path)\n",
        "test_texts, test_labels = load_data(test_data_path)\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def tokenize(texts):\n",
        "    return [enc.encode_ordinary(text) for text in texts]\n",
        "\n",
        "# Tokenize the train, validation, and test datasets\n",
        "train_ids = tokenize(train_texts)\n",
        "val_ids = tokenize(val_texts)\n",
        "test_ids = tokenize(test_texts)\n",
        "\n",
        "# Padding or truncation to block_size\n",
        "block_size = 1024\n",
        "def pad_or_truncate(data, block_size):\n",
        "    return [seq[:block_size] if len(seq) > block_size else seq + [0] * (block_size - len(seq)) for seq in data]\n",
        "\n",
        "train_ids = pad_or_truncate(train_ids, block_size)\n",
        "val_ids = pad_or_truncate(val_ids, block_size)\n",
        "test_ids = pad_or_truncate(test_ids, block_size)\n",
        "\n",
        "# Convert to tensors\n",
        "train_ids = torch.tensor(train_ids)\n",
        "val_ids = torch.tensor(val_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "# Convert labels to numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder on the training labels and transform all labels\n",
        "train_labels = label_encoder.fit_transform(train_labels)\n",
        "val_labels = label_encoder.transform(val_labels)\n",
        "test_labels = label_encoder.transform(test_labels)\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for train and validation sets\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "val_dataset = TensorDataset(val_ids, val_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "# Set the batch size\n",
        "batch_size = 12\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "OMxrH4Hb0NOX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(torch.tril(torch.ones(T, T, device=x.device)) == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        # Use the last token's representation for classification into 3 classes\n",
        "        self.classifier_head = nn.Linear(config.n_embd, 3)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        pos = torch.arange(0, idx.size(1), dtype=torch.long, device=idx.device)\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        # Use only the final token's embedding for classification\n",
        "        logits = self.classifier_head(x[:, -1, :])  # shape (b, 3)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "uWQzdsNp1vr6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"sentiment_analysis_project\", name=\"nanogpt_classification_run\")\n",
        "\n",
        "# I/O\n",
        "# Modify the values\n",
        "out_dir = 'out'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "eval_interval = 50\n",
        "log_interval = 50\n",
        "# eval_iters = 200\n",
        "max_iters = 1000\n",
        "always_save_checkpoint = True\n",
        "init_from = 'scratch'\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "# batch_size = 12\n",
        "block_size = 1024\n",
        "\n",
        "# Modifiy and set the number of layers and  heads as well as the size of embeddings\n",
        "n_layer = 4\n",
        "n_head = 4\n",
        "n_embd = 256\n",
        "\n",
        "dropout = 0.1\n",
        "bias = True\n",
        "\n",
        "learning_rate = 6e-4\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "\n",
        "# remove the following\n",
        "# decay_lr = True\n",
        "# warmup_iters = 2000\n",
        "# lr_decay_iters = 600000\n",
        "# min_lr = 6e-5\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# Initialize model\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=50304, dropout=dropout)\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
        "scaler = GradScaler()\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(val_loader)\n",
        "\n",
        "def get_batch(split):\n",
        "    global train_iter, val_iter\n",
        "    try:\n",
        "        if split == 'train':\n",
        "            x, y = next(train_iter)\n",
        "        else:\n",
        "            x, y = next(val_iter)\n",
        "    except StopIteration:\n",
        "        if split == 'train':\n",
        "            train_iter = iter(train_loader)\n",
        "            x, y = next(train_iter)\n",
        "        else:\n",
        "            val_iter = iter(val_loader)\n",
        "            x, y = next(val_iter)\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
        "        losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "        out[f\"{split}_acc\"] = correct / total\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Early stopping setup\n",
        "val_loss_history = []\n",
        "early_stop_patience = 10\n",
        "\n",
        "# Training loop\n",
        "iter_num = 0\n",
        "best_val_loss = float('inf')\n",
        "t0 = time.time()\n",
        "\n",
        "while iter_num <= max_iters:\n",
        "    lr = learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, train acc {losses['train_acc']:.4f}, val loss {val_loss:.4f}, val acc {losses['val_acc']:.4f}\")\n",
        "        wandb.log({\n",
        "            \"train/loss\": losses['train'],\n",
        "            \"train/accuracy\": losses['train_acc'],\n",
        "            \"val/loss\": val_loss,\n",
        "            \"val/accuracy\": losses['val_acc'],\n",
        "            \"step\": iter_num\n",
        "        })\n",
        "\n",
        "        # Early stopping condition\n",
        "        val_loss_history.append(val_loss)\n",
        "        if len(val_loss_history) > early_stop_patience:\n",
        "            val_loss_history.pop(0)\n",
        "            if all(val_loss >= prev for prev in val_loss_history):\n",
        "                print(f\"Early stopping: No improvement in val loss for {early_stop_patience} evaluations.\")\n",
        "                break\n",
        "\n",
        "        if val_loss < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = val_loss\n",
        "            checkpoint = {\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'model_args': model_args,\n",
        "                'iter_num': iter_num,\n",
        "                'best_val_loss': best_val_loss,\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    # Forward/backward\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        x, y = get_batch('train')\n",
        "        with ctx:\n",
        "            logits, loss = model(x, y)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if iter_num % log_interval == 0:\n",
        "        print(f\"iter {iter_num}: loss {loss.item() * gradient_accumulation_steps:.4f}, time {(time.time()-t0)*1000:.2f}ms\")\n",
        "        wandb.log({\"train/batch_loss\": loss.item() * gradient_accumulation_steps, \"step\": iter_num})\n",
        "        t0 = time.time()\n",
        "\n",
        "    iter_num += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "43gkcmjtmkcv",
        "outputId": "8c0a75e8-4883-47b5-ba7d-704ce51f7398"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250404_182847-rdu2jial</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aeren/sentiment_analysis_project/runs/rdu2jial' target=\"_blank\">nanogpt_classification_run</a></strong> to <a href='https://wandb.ai/aeren/sentiment_analysis_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aeren/sentiment_analysis_project' target=\"_blank\">https://wandb.ai/aeren/sentiment_analysis_project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aeren/sentiment_analysis_project/runs/rdu2jial' target=\"_blank\">https://wandb.ai/aeren/sentiment_analysis_project/runs/rdu2jial</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ff58bddb3f93>:65: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 0.9046, train acc 0.5606, val loss 0.9153, val acc 0.5515\n",
            "iter 0: loss 0.8232, time 2250.64ms\n",
            "step 50: train loss 0.7748, train acc 0.6675, val loss 0.8865, val acc 0.5979\n",
            "iter 50: loss 0.6916, time 4535.03ms\n",
            "step 100: train loss 0.4067, train acc 0.8415, val loss 0.5074, val acc 0.7732\n",
            "iter 100: loss 0.4979, time 4532.54ms\n",
            "step 150: train loss 0.5138, train acc 0.8428, val loss 0.6844, val acc 0.7680\n",
            "iter 150: loss 0.2901, time 4424.44ms\n",
            "step 200: train loss 0.2324, train acc 0.9304, val loss 0.4107, val acc 0.8608\n",
            "iter 200: loss 0.0426, time 4495.04ms\n",
            "step 250: train loss 0.1999, train acc 0.9433, val loss 0.4675, val acc 0.8402\n",
            "iter 250: loss 0.0599, time 4521.47ms\n",
            "step 300: train loss 0.1530, train acc 0.9601, val loss 0.3908, val acc 0.8454\n",
            "iter 300: loss 0.0771, time 4500.86ms\n",
            "step 350: train loss 0.1338, train acc 0.9652, val loss 0.5697, val acc 0.8608\n",
            "iter 350: loss 0.0391, time 4472.36ms\n",
            "step 400: train loss 0.1212, train acc 0.9665, val loss 0.5783, val acc 0.8608\n",
            "iter 400: loss 0.0383, time 4486.71ms\n",
            "step 450: train loss 0.0804, train acc 0.9807, val loss 0.4569, val acc 0.8866\n",
            "iter 450: loss 0.2441, time 4514.25ms\n",
            "step 500: train loss 0.1013, train acc 0.9691, val loss 0.9764, val acc 0.8351\n",
            "Early stopping: No improvement in val loss for 10 evaluations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score, classification_report\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        test_texts, labels = batch\n",
        "        test_texts, labels = test_texts.to(device), labels.to(device)\n",
        "\n",
        "        logits, _ = model(test_texts)\n",
        "        predicted_labels = logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        predictions.extend(predicted_labels)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Overall accuracy\n",
        "overall_accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"\\nOverall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Class-wise accuracy\n",
        "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "print(\"\\nClass-wise Accuracy:\")\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    print(f\"  Class {i} ({label_encoder.classes_[i]}): {acc * 100:.2f}%\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
        "print(report)\n",
        "\n",
        "# Print confusion matrix\n",
        "ordered_labels = ['negative', 'neutral', 'positive']\n",
        "ordered_indices = [list(label_encoder.classes_).index(label) for label in ordered_labels]\n",
        "\n",
        "reordered_conf_matrix = conf_matrix[np.ix_(ordered_indices, ordered_indices)]\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix (as table):\")\n",
        "print(\"{:<10}\".format(\"\"), end=\"\")\n",
        "for label in ordered_labels:\n",
        "    print(\"{:<10}\".format(label), end=\"\")\n",
        "print()\n",
        "\n",
        "for i, row_idx in enumerate(ordered_indices):\n",
        "    row_label = ordered_labels[i]\n",
        "    print(\"{:<10}\".format(row_label), end=\"\")\n",
        "    for col_idx in ordered_indices:\n",
        "        print(\"{:<10}\".format(conf_matrix[row_idx][col_idx]), end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Heatmap visualization for confusion matrix.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zH4yBd2d_OSc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2bea8fc-abc6-4ede-de52-0d2e80093ccd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Accuracy: 50.00%\n",
            "\n",
            "Class-wise Accuracy:\n",
            "  Class 0 (negative): 80.00%\n",
            "  Class 1 (neutral): 70.00%\n",
            "  Class 2 (positive): 0.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.73      0.80      0.76        10\n",
            "     neutral       0.37      0.70      0.48        10\n",
            "    positive       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.50        30\n",
            "   macro avg       0.37      0.50      0.41        30\n",
            "weighted avg       0.37      0.50      0.41        30\n",
            "\n",
            "\n",
            "Confusion Matrix (as table):\n",
            "          negative  neutral   positive  \n",
            "negative  8         2         0         \n",
            "neutral   3         7         0         \n",
            "positive  0         10        0         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}